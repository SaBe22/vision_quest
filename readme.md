# Welcome to Vision Quest
This repository provides a collection of simple and easy-to-use PyTorch implementations of various vision models.

### Implemented models:

* **Vision Transformers (ViT)**: Vision transformers implementations inspired by ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"](https://arxiv.org/abs/2010.11929) paper.
* **Masked Autoencoders (MAE)**: Unsupervised learning with MAE models based on ["Masked Autoencoders Are Scalable Vision Learners"](https://arxiv.org/abs/2111.06377)
* **Swin Transformer (Swin)**: Swin transformer implementation based on ["Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"](https://arxiv.org/abs/2103.14030)
* **Multi-Axis ViT (MaxViT)**: MaxViT implementation based on ["MaxViT: Multi-Axis Vision Transformer"](https://arxiv.org/abs/2204.01697)
* **CoAtNet**: CoAtNet implementation based on [CoAtNet: Marrying Convolution and Attention](https://arxiv.org/abs/2106.04803)

Stay tuned for more! I'm constantly expanding my collection to include new and exciting vision models.